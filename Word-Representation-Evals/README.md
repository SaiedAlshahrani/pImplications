#     Word Representation Evaluations

We evaluate the **performance** of the Word Embedding Models using the Word Analogy Evaluation Task and our [Arab States Analogy Dataset (ASAD)](https://github.com/SaiedAlshahrani/performance-implications/tree/main/Word-Representation-Evals/ASAD) dataset. To measure the **impact of template-based translation**, we compare the performance of models trained on the Egyptian Arabic Wikipedia edition’s corpora, which are dominated by template-based translation, to the performance of models trained on Modern Standard Arabic and Moroccan Arabic Wikipedia editions’ corpora, which are not. On the other hand, to measure the **impact of bot-based generation**, we compare the performance of word embedding models trained on Arabic and Moroccan Arabic corpora (with and without bot-generated articles) using the word analogy task and our [ASAD](https://github.com/SaiedAlshahrani/performance-implications/tree/main/Word-Representation-Evals/ASAD) dataset.

> We have trained **35** context-independent Word Embedding Models for this experiment, and due to the huge size of these models, we could not share them with the community. However, we share our [training scripts](https://github.com/SaiedAlshahrani/performance-implications/tree/main/Word-Representation-Evals/Training-Scripts) for these models and provide detailed documentation on the creation of Wikipedia corpora that can be found here at [Wikipedia-Corpora-Creation](https://github.com/SaiedAlshahrani/performance-implications/tree/main/Wikipedia-Corpora-Creation). 

