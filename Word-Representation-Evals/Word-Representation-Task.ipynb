{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa07e6a0-f7f6-4ce0-8c24-eb909fcf4128",
   "metadata": {},
   "source": [
    "# __Word Analogy Evaluations of Word Representation Task__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d3765-3db1-46fd-bda7-297a0a1340e3",
   "metadata": {},
   "source": [
    "## # Evaluation of __Arabic Wikipedia__ Word Embedding Models (With Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b435a673-82b7-436f-ba90-5681a1c5392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH BOTS):\n",
      "   @ MODEL: arwiki_20230101_w2v_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 53.82%\n",
      "   @ MODEL: arwiki_20230101_w2v_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 53.88%\n",
      "   @ MODEL: arwiki_20230101_fasttext_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 21.97%\n",
      "   @ MODEL: arwiki_20230101_fasttext_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 39.67%\n",
      "   @ MODEL: arwiki_20230101_bots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 36.58%\n",
      "   @ MODEL: arwiki_20230101_w2v_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 71.91%\n",
      "   @ MODEL: arwiki_20230101_w2v_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 74.47%\n",
      "   @ MODEL: arwiki_20230101_fasttext_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 34.67%\n",
      "   @ MODEL: arwiki_20230101_fasttext_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 57.17%\n",
      "   @ MODEL: arwiki_20230101_bots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 50.53%\n",
      "   @ MODEL: arwiki_20230101_w2v_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 76.64%\n",
      "   @ MODEL: arwiki_20230101_w2v_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 79.67%\n",
      "   @ MODEL: arwiki_20230101_fasttext_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 44.47%\n",
      "   @ MODEL: arwiki_20230101_fasttext_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 65.79%\n",
      "   @ MODEL: arwiki_20230101_bots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=2,122,484, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 54.14%\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from embeddings_utils import *\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "w2v_models = [str(x) for x in Path('Word-Embeddings/Word2Vec-Models/ar/').glob('arwiki_20230101_w2v_*_bots.model')]\n",
    "glove_models = [str(x) for x in Path('Word-Embeddings/GloVe-Models/ar/').glob('arwiki*_bots_glove.vectors')]\n",
    "fasttext_models = [str(x) for x in Path('Word-Embeddings/fastText-Models/ar/').glob('arwiki_20230101_fasttext_*_bots.model')]\n",
    "\n",
    "models = w2v_models + fasttext_models + glove_models\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('ASAD/Arab_States_Analogy_Dataset_All.txt')]\n",
    "\n",
    "print(f\"## EVALUATION OF ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH BOTS):\")\n",
    "\n",
    "top_ks=[1, 5, 10]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "\n",
    "        if 'glove' in model_name: model = load_model(model, 'glove')\n",
    "        else: model = load_model(model)\n",
    "\n",
    "        if 'glove' in model_name:\n",
    "            model_params = f'<vocab_size={format(len(model.key_to_index)-1, \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "        else:\n",
    "            model_params = f'<vocab_size={format(len(model.wv.key_to_index), \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            analogy_test_set = pd.read_csv(file, sep=\" \", header=None)\n",
    "            analogy_test_set.columns = [\"example1\", \"example2\", \"query\", \"answer\"]\n",
    "\n",
    "            if 'glove' in model_name:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, model_format='glove', top_k=top_k, axis=1)\n",
    "            else:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, top_k=top_k, axis=1)\n",
    "\n",
    "            analogy_test_set['is_accurate'] = analogy_test_set.apply(lambda row: 1 if row.answer in row.pred_answer else 0, axis=1)\n",
    "            accuracy = analogy_test_set['is_accurate'].sum()/len(analogy_test_set) * 100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arwiki_wem_results_bots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a6b1e-983c-43bc-9809-0b59067bd18a",
   "metadata": {},
   "source": [
    "## # Evaluation of __Arabic Wikipedia__ Word Embedding Models (With No Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58806827-81eb-4f1b-aa9f-3d95f041e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH NO BOTS):\n",
      "   @ MODEL: arwiki_20230101_w2v_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 54.47%\n",
      "   @ MODEL: arwiki_20230101_w2v_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 53.22%\n",
      "   @ MODEL: arwiki_20230101_fasttext_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 39.87%\n",
      "   @ MODEL: arwiki_20230101_fasttext_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 22.76%\n",
      "   @ MODEL: arwiki_20230101_nobots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 38.29%\n",
      "   @ MODEL: arwiki_20230101_w2v_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 71.84%\n",
      "   @ MODEL: arwiki_20230101_w2v_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 74.47%\n",
      "   @ MODEL: arwiki_20230101_fasttext_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 56.64%\n",
      "   @ MODEL: arwiki_20230101_fasttext_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 34.34%\n",
      "   @ MODEL: arwiki_20230101_nobots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 52.11%\n",
      "   @ MODEL: arwiki_20230101_w2v_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 75.92%\n",
      "   @ MODEL: arwiki_20230101_w2v_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 79.47%\n",
      "   @ MODEL: arwiki_20230101_fasttext_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 67.43%\n",
      "   @ MODEL: arwiki_20230101_fasttext_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 43.29%\n",
      "   @ MODEL: arwiki_20230101_nobots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=2,102,375, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 55.13%\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from embeddings_utils import *\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "w2v_models = [str(x) for x in Path('Word-Embeddings/Word2Vec-Models/ar/').glob('arwiki_20230101_w2v_*_nobots.model')]\n",
    "glove_models = [str(x) for x in Path('Word-Embeddings/GloVe-Models/ar/').glob('arwiki*_nobots_glove.vectors')]\n",
    "fasttext_models = [str(x) for x in Path('Word-Embeddings/fastText-Models/ar/').glob('arwiki_20230101_fasttext_*_nobots.model')]\n",
    "\n",
    "models = w2v_models + fasttext_models + glove_models \n",
    "\n",
    "files = [str(x) for x in Path('.').glob('ASAD/Arab_States_Analogy_Dataset_All.txt')]\n",
    "\n",
    "print(f\"## EVALUATION OF ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH NO BOTS):\")\n",
    "\n",
    "top_ks=[1, 5, 10]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "\n",
    "        if 'glove' in model_name: model = load_model(model, 'glove')\n",
    "        else: model = load_model(model)\n",
    "\n",
    "        if 'glove' in model_name:\n",
    "            model_params = f'<vocab_size={format(len(model.key_to_index)-1, \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "        else:\n",
    "            model_params = f'<vocab_size={format(len(model.wv.key_to_index), \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            analogy_test_set = pd.read_csv(file, sep=\" \", header=None)\n",
    "            analogy_test_set.columns = [\"example1\", \"example2\", \"query\", \"answer\"]\n",
    "\n",
    "            if 'glove' in model_name:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, model_format='glove', top_k=top_k, axis=1)\n",
    "            else:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, top_k=top_k, axis=1)\n",
    "\n",
    "            analogy_test_set['is_accurate'] = analogy_test_set.apply(lambda row: 1 if row.answer in row.pred_answer else 0, axis=1)\n",
    "            accuracy = analogy_test_set['is_accurate'].sum()/len(analogy_test_set) * 100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arwiki_wem_results_nobots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee809a-fec2-49d6-ada5-b9dd1c73362c",
   "metadata": {},
   "source": [
    "## # Evaluation of __Egyptian Arabic Wikipedia__ Word Embedding Models (*only* With Bots):\n",
    "We dropped the Egyptian Arabic Wikipedia (_from the evaluation with no bots)_ due to having an insignificant number of bot-generated articles, only 15 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99048bcc-99a5-4b12-8feb-c00ea4beffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF EGYPTIAN ARABIC WIKIPEDIA WORD EMBEDDING MODELS (ONLY WITH BOTS):\n",
      "   @ MODEL: arzwiki_20230101_w2v_cbow.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 13.88%\n",
      "   @ MODEL: arzwiki_20230101_w2v_skipgram.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 5.0%\n",
      "   @ MODEL: arzwiki_20230101_fasttext_skipgram.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 10.13%\n",
      "   @ MODEL: arzwiki_20230101_fasttext_cbow.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 11.64%\n",
      "   @ MODEL: arzwiki_20230101_glove.vectors\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 0.53%\n",
      "   @ MODEL: arzwiki_20230101_w2v_cbow.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 26.97%\n",
      "   @ MODEL: arzwiki_20230101_w2v_skipgram.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 9.08%\n",
      "   @ MODEL: arzwiki_20230101_fasttext_skipgram.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 20.86%\n",
      "   @ MODEL: arzwiki_20230101_fasttext_cbow.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 18.22%\n",
      "   @ MODEL: arzwiki_20230101_glove.vectors\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 3.29%\n",
      "   @ MODEL: arzwiki_20230101_w2v_cbow.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 33.09%\n",
      "   @ MODEL: arzwiki_20230101_w2v_skipgram.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 11.05%\n",
      "   @ MODEL: arzwiki_20230101_fasttext_skipgram.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 28.09%\n",
      "   @ MODEL: arzwiki_20230101_fasttext_cbow.model\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 22.37%\n",
      "   @ MODEL: arzwiki_20230101_glove.vectors\n",
      "   @ PARAMS: <vocab_size=487,466, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 5.2%\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from embeddings_utils import *\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "w2v_models = [str(x) for x in Path('Word-Embeddings/Word2Vec-Models/arz/').glob('arzwiki_20230101_w2v_*.model')]\n",
    "glove_models = [str(x) for x in Path('Word-Embeddings/GloVe-Models/arz/').glob('arzwiki_*_glove.vectors')]\n",
    "fasttext_models = [str(x) for x in Path('Word-Embeddings/fastText-Models/arz/').glob('arzwiki_20230101_fasttext_*.model')]\n",
    "\n",
    "models = w2v_models + fasttext_models + glove_models\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('ASAD/Arab_States_Analogy_Dataset_All.txt')]\n",
    "\n",
    "print(f\"## EVALUATION OF EGYPTIAN ARABIC WIKIPEDIA WORD EMBEDDING MODELS (ONLY WITH BOTS):\")\n",
    "\n",
    "top_ks=[1, 5, 10]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "\n",
    "        if 'glove' in model_name: model = load_model(model, 'glove')\n",
    "        else: model = load_model(model)\n",
    "\n",
    "        if 'glove' in model_name:\n",
    "            model_params = f'<vocab_size={format(len(model.key_to_index)-1, \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "        else:\n",
    "            model_params = f'<vocab_size={format(len(model.wv.key_to_index), \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            analogy_test_set = pd.read_csv(file, sep=\" \", header=None)\n",
    "            analogy_test_set.columns = [\"example1\", \"example2\", \"query\", \"answer\"]\n",
    "\n",
    "            if 'glove' in model_name:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, model_format='glove', top_k=top_k, axis=1)\n",
    "            else:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, top_k=top_k, axis=1)\n",
    "\n",
    "            analogy_test_set['is_accurate'] = analogy_test_set.apply(lambda row: 1 if row.answer in row.pred_answer else 0, axis=1)\n",
    "            accuracy = analogy_test_set['is_accurate'].sum()/len(analogy_test_set) * 100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arzwiki_wem_results_bots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f493ad-7268-47e3-9559-d9cafd206ba5",
   "metadata": {},
   "source": [
    "## # Evaluation of __Moroccan Arabic Wikipedia__ Word Embedding Models (With Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a03d070-de07-4401-924e-820b911462d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH BOTS):\n",
      "   @ MODEL: arywiki_20230101_w2v_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 1.91%\n",
      "   @ MODEL: arywiki_20230101_w2v_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 2.11%\n",
      "   @ MODEL: arywiki_20230101_fasttext_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 1.71%\n",
      "   @ MODEL: arywiki_20230101_fasttext_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 3.68%\n",
      "   @ MODEL: arywiki_20230101_bots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 0.13%\n",
      "   @ MODEL: arywiki_20230101_w2v_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 5.86%\n",
      "   @ MODEL: arywiki_20230101_w2v_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 4.01%\n",
      "   @ MODEL: arywiki_20230101_fasttext_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 4.41%\n",
      "   @ MODEL: arywiki_20230101_fasttext_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 9.87%\n",
      "   @ MODEL: arywiki_20230101_bots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 0.53%\n",
      "   @ MODEL: arywiki_20230101_w2v_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 8.22%\n",
      "   @ MODEL: arywiki_20230101_w2v_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 5.92%\n",
      "   @ MODEL: arywiki_20230101_fasttext_cbow_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 6.38%\n",
      "   @ MODEL: arywiki_20230101_fasttext_skipgram_bots.model\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 14.61%\n",
      "   @ MODEL: arywiki_20230101_bots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=72,467, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 0.66%\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from embeddings_utils import *\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "w2v_models = [str(x) for x in Path('Word-Embeddings/Word2Vec-Models/ary/').glob('arywiki_20230101_w2v_*_bots.model')]\n",
    "glove_models = [str(x) for x in Path('Word-Embeddings/GloVe-Models/ary/').glob('arywiki*_bots_glove.vectors')]\n",
    "fasttext_models = [str(x) for x in Path('Word-Embeddings/fastText-Models/ary/').glob('arywiki_20230101_fasttext_*_bots.model')]\n",
    "\n",
    "models = w2v_models + fasttext_models + glove_models\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('ASAD/Arab_States_Analogy_Dataset_All.txt')]\n",
    "\n",
    "print(f\"## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH BOTS):\")\n",
    "\n",
    "top_ks=[1, 5, 10]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "\n",
    "        if 'glove' in model_name: model = load_model(model, 'glove')\n",
    "        else: model = load_model(model)\n",
    "\n",
    "        if 'glove' in model_name:\n",
    "            model_params = f'<vocab_size={format(len(model.key_to_index)-1, \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "        else:\n",
    "            model_params = f'<vocab_size={format(len(model.wv.key_to_index), \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            analogy_test_set = pd.read_csv(file, sep=\" \", header=None)\n",
    "            analogy_test_set.columns = [\"example1\", \"example2\", \"query\", \"answer\"]\n",
    "\n",
    "            if 'glove' in model_name:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, model_format='glove', top_k=top_k, axis=1)\n",
    "            else:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, top_k=top_k, axis=1)\n",
    "\n",
    "            analogy_test_set['is_accurate'] = analogy_test_set.apply(lambda row: 1 if row.answer in row.pred_answer else 0, axis=1)\n",
    "            accuracy = analogy_test_set['is_accurate'].sum()/len(analogy_test_set) * 100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arywiki_wem_results_bots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98abeb-fb74-49ff-a319-32860c8ede17",
   "metadata": {},
   "source": [
    "## # Evaluation of __Moroccan Arabic Wikipedia__ Word Embedding Models (With No Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee44acd-09e8-4a6a-82b2-eb6221a5c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH NO BOTS):\n",
      "   @ MODEL: arywiki_20230101_w2v_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 2.11%\n",
      "   @ MODEL: arywiki_20230101_w2v_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 1.84%\n",
      "   @ MODEL: arywiki_20230101_fasttext_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 3.62%\n",
      "   @ MODEL: arywiki_20230101_fasttext_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 1.97%\n",
      "   @ MODEL: arywiki_20230101_nobots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=1>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 0.07%\n",
      "   @ MODEL: arywiki_20230101_w2v_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 3.75%\n",
      "   @ MODEL: arywiki_20230101_w2v_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 4.54%\n",
      "   @ MODEL: arywiki_20230101_fasttext_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 9.54%\n",
      "   @ MODEL: arywiki_20230101_fasttext_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 4.41%\n",
      "   @ MODEL: arywiki_20230101_nobots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=5>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 0.26%\n",
      "   @ MODEL: arywiki_20230101_w2v_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 5.53%\n",
      "   @ MODEL: arywiki_20230101_w2v_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 7.11%\n",
      "   @ MODEL: arywiki_20230101_fasttext_skipgram_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 13.75%\n",
      "   @ MODEL: arywiki_20230101_fasttext_cbow_nobots.model\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 6.45%\n",
      "   @ MODEL: arywiki_20230101_nobots_glove.vectors\n",
      "   @ PARAMS: <vocab_size=71,704, vector_size=300, top_k=10>\n",
      "     * FILE: Arab_States_Analogy_Dataset_All.txt\n",
      "       > ACCURACY: 0.39%\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from embeddings_utils import *\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "w2v_models = [str(x) for x in Path('Word-Embeddings/Word2Vec-Models/ary/').glob('arywiki_20230101_w2v_*_nobots.model')]\n",
    "glove_models = [str(x) for x in Path('Word-Embeddings/GloVe-Models/ary/').glob('arywiki*_nobots_glove.vectors')]\n",
    "fasttext_models = [str(x) for x in Path('Word-Embeddings/fastText-Models/ary/').glob('arywiki_20230101_fasttext_*_nobots.model')]\n",
    "\n",
    "models = w2v_models + fasttext_models + glove_models\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('ASAD/Arab_States_Analogy_Dataset_All.txt')]\n",
    "\n",
    "print(f\"## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA WORD EMBEDDING MODELS (WITH NO BOTS):\")\n",
    "\n",
    "top_ks=[1, 5, 10]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "\n",
    "        if 'glove' in model_name: model = load_model(model, 'glove')\n",
    "        else: model = load_model(model)\n",
    "\n",
    "        if 'glove' in model_name:\n",
    "            model_params = f'<vocab_size={format(len(model.key_to_index)-1, \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "        else:\n",
    "            model_params = f'<vocab_size={format(len(model.wv.key_to_index), \",d\")}, vector_size={model.vector_size}, top_k={top_k}>'\n",
    "            print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            analogy_test_set = pd.read_csv(file, sep=\" \", header=None)\n",
    "            analogy_test_set.columns = [\"example1\", \"example2\", \"query\", \"answer\"]\n",
    "\n",
    "            if 'glove' in model_name:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, model_format='glove', top_k=top_k, axis=1)\n",
    "            else:\n",
    "                analogy_test_set['pred_answer'] = analogy_test_set.apply(get_analogy_by_row, model=model, top_k=top_k, axis=1)\n",
    "\n",
    "            analogy_test_set['is_accurate'] = analogy_test_set.apply(lambda row: 1 if row.answer in row.pred_answer else 0, axis=1)\n",
    "            accuracy = analogy_test_set['is_accurate'].sum()/len(analogy_test_set) * 100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arywiki_wem_results_nobots.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (mldl)",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
