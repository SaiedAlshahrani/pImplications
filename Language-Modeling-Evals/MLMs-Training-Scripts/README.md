# MLMs Training Scripts
We share our Python training scripts of our **RoBERTa<sub>BASE</sub>** (*trained from scratch*). We have trained at least *one* Masked Language Models (MLMs) for each Arabic Wikipedia edition (Modern Standard Arabic, Egyptian Arabic, and Moroccan Arabic) with one modification on their architectures. We set the number of hidden layers to **6** instead of **12** for less computational overhead and to make the MLM models twice as fast as the **RoBERTa<sub>BASE</sub>** introduced by [Liu et al. (2019)](https://arxiv.org/abs/1907.11692). Alongside that, we also train three Byte-level Byte-Pair-Encoding (BPE) tokenizers, one for each Arabic Wikipedia editionâ€™s corpora. 

> We *only* share our script of the Moroccan Arabic MLM model (**aryRoBERTa<sub>BASE</sub>**) to avoid repetition here at [aryRoBERTa.ipynb](https://github.com/SaiedAlshahrani/performance-implications/blob/main/Language-Modeling-Evals/MLMs-Training-Scripts/aryRoBERTa.ipynb).
