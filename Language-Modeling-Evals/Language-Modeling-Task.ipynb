{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa07e6a0-f7f6-4ce0-8c24-eb909fcf4128",
   "metadata": {},
   "source": [
    "# __Fill-Mask Evaluations of Language Modeling Task__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03315749-fa00-4b49-bb68-f9e8453b318a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## # Evaluation of __Arabic Wikipedia__ Masked Language Model (With Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a03d070-de07-4401-924e-820b911462d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH BOTS):\n",
      "   @ MODEL: arwiki_20230101_roberta_mlm_bots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=10>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 43.12%\n",
      "   @ MODEL: arwiki_20230101_roberta_mlm_bots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=50>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 45.0%\n",
      "   @ MODEL: arwiki_20230101_roberta_mlm_bots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=100>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 50.62%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlm_utils import *\n",
    "from pathlib import Path\n",
    "import os, logging, warnings\n",
    "from transformers import pipeline\n",
    "from transformers import logging as hflogging\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "\n",
    "disable_progress_bars()\n",
    "hflogging.set_verbosity_error()\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "models = ['SaiedAlshahrani/arwiki_20230101_roberta_mlm_bots']\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('MASD/Masked_Arab_States_Dataset_All.csv')]\n",
    "\n",
    "print(f\"## EVALUATION OF ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH BOTS):\")\n",
    "\n",
    "top_ks=[10, 50, 100]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "        \n",
    "        model_params = f'<vocab_size={format(get_model_config(model)[\"vocab_size\"],\",d\")}, vector_size={get_model_config(model)[\"max_position_embeddings\"]}, top_k={top_k}>'\n",
    "        print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            masked_test_set = pd.read_csv(file)\n",
    "            masked_test_set['pred_masked_tokens'] = masked_test_set.apply(lambda col: mask_filler(col['mlm_prompt'], model, top_k), axis=1)\n",
    "            masked_test_set['is_accurate'] = masked_test_set.apply(lambda row: 1 if row.masked_token in row.pred_masked_tokens else 0, axis=1)\n",
    "            accuracy = (masked_test_set['is_accurate'].sum()/masked_test_set.shape[0])*100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arwiki_mlm_results_bots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247f516-cbb0-4015-94e1-5a7fb3c79813",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## # Evaluation of __Arabic Wikipedia__ Masked Language Model (With No Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c2486c-6325-4154-af8e-bd6fd8e77991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH NO BOTS):\n",
      "   @ MODEL: arwiki_20230101_roberta_mlm_nobots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=10>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 45.62%\n",
      "   @ MODEL: arwiki_20230101_roberta_mlm_nobots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=50>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 51.25%\n",
      "   @ MODEL: arwiki_20230101_roberta_mlm_nobots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=100>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 53.12%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlm_utils import *\n",
    "from pathlib import Path\n",
    "import os, logging, warnings\n",
    "from transformers import pipeline\n",
    "from transformers import logging as hflogging\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "\n",
    "disable_progress_bars()\n",
    "hflogging.set_verbosity_error()\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "models = ['SaiedAlshahrani/arwiki_20230101_roberta_mlm_nobots']\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('MASD/Masked_Arab_States_Dataset_All.csv')]\n",
    "\n",
    "print(f\"## EVALUATION OF ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH NO BOTS):\")\n",
    "\n",
    "top_ks=[10, 50, 100]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "        \n",
    "        model_params = f'<vocab_size={format(get_model_config(model)[\"vocab_size\"],\",d\")}, vector_size={get_model_config(model)[\"max_position_embeddings\"]}, top_k={top_k}>'\n",
    "        print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            masked_test_set = pd.read_csv(file)\n",
    "            masked_test_set['pred_masked_tokens'] = masked_test_set.apply(lambda col: mask_filler(col['mlm_prompt'], model, top_k), axis=1)\n",
    "            masked_test_set['is_accurate'] = masked_test_set.apply(lambda row: 1 if row.masked_token in row.pred_masked_tokens else 0, axis=1)\n",
    "            accuracy = (masked_test_set['is_accurate'].sum()/masked_test_set.shape[0])*100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arwiki_mlm_results_nobots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32b9ff-f6f1-4208-8673-23a91410c98d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## # Evaluation of __Egyptian Arabic Wikipedia__ Masked Language Model (_only_ With Bots):\n",
    "We dropped the Egyptian Arabic Wikipedia (_from the evaluation with no bots_) due to having an insignificant number of bot-generated articles, only 15 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921602d3-bfe6-4577-817d-8c724acc90f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF EGYPTIAN ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (ONLY WITH BOTS):\n",
      "   @ MODEL: arzwiki_20230101_roberta_mlm\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=10>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 8.12%\n",
      "   @ MODEL: arzwiki_20230101_roberta_mlm\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=50>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 25.62%\n",
      "   @ MODEL: arzwiki_20230101_roberta_mlm\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=100>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 35.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlm_utils import *\n",
    "from pathlib import Path\n",
    "import os, logging, warnings\n",
    "from transformers import pipeline\n",
    "from transformers import logging as hflogging\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "\n",
    "disable_progress_bars()\n",
    "hflogging.set_verbosity_error()\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "models = ['SaiedAlshahrani/arzwiki_20230101_roberta_mlm']\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('MASD/Masked_Arab_States_Dataset_All.csv')]\n",
    "\n",
    "print(f\"## EVALUATION OF EGYPTIAN ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (ONLY WITH BOTS):\")\n",
    "\n",
    "top_ks=[10, 50, 100]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "        \n",
    "        model_params = f'<vocab_size={format(get_model_config(model)[\"vocab_size\"],\",d\")}, vector_size={get_model_config(model)[\"max_position_embeddings\"]}, top_k={top_k}>'\n",
    "        print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            masked_test_set = pd.read_csv(file)\n",
    "            masked_test_set['pred_masked_tokens'] = masked_test_set.apply(lambda col: mask_filler(col['mlm_prompt'], model, top_k), axis=1)\n",
    "            masked_test_set['is_accurate'] = masked_test_set.apply(lambda row: 1 if row.masked_token in row.pred_masked_tokens else 0, axis=1)\n",
    "            accuracy = (masked_test_set['is_accurate'].sum()/masked_test_set.shape[0])*100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arzwiki_mlm_results_bots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788161f9-f5ec-4899-8010-38064fb2b67d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## # Evaluation of __Moroccan Arabic Wikipedia__ Masked Language Model (With Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20aca205-e789-4e38-9179-17e0cd7e9531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH BOTS):\n",
      "   @ MODEL: arywiki_20230101_roberta_mlm_bots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=10>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 0.0%\n",
      "   @ MODEL: arywiki_20230101_roberta_mlm_bots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=50>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 0.0%\n",
      "   @ MODEL: arywiki_20230101_roberta_mlm_bots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=100>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 0.62%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlm_utils import *\n",
    "from pathlib import Path\n",
    "import os, logging, warnings\n",
    "from transformers import pipeline\n",
    "from transformers import logging as hflogging\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "\n",
    "disable_progress_bars()\n",
    "hflogging.set_verbosity_error()\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "models = ['SaiedAlshahrani/arywiki_20230101_roberta_mlm_bots']\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('MASD/Masked_Arab_States_Dataset_All.csv')]\n",
    "\n",
    "print(f\"## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH BOTS):\")\n",
    "\n",
    "top_ks=[10, 50, 100]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "        \n",
    "        model_params = f'<vocab_size={format(get_model_config(model)[\"vocab_size\"],\",d\")}, vector_size={get_model_config(model)[\"max_position_embeddings\"]}, top_k={top_k}>'\n",
    "        print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            masked_test_set = pd.read_csv(file)\n",
    "            masked_test_set['pred_masked_tokens'] = masked_test_set.apply(lambda col: mask_filler(col['mlm_prompt'], model, top_k), axis=1)\n",
    "            masked_test_set['is_accurate'] = masked_test_set.apply(lambda row: 1 if row.masked_token in row.pred_masked_tokens else 0, axis=1)\n",
    "            accuracy = (masked_test_set['is_accurate'].sum()/masked_test_set.shape[0])*100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arywiki_mlm_results_bots.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d7942-aee1-43fd-a091-d0d780fa56ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## # Evaluation of __Moroccan Arabic Wikipedia__ Masked Language Model (With No Bots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9cbaa1-4fcb-4182-9fee-1a0845bd67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH NO BOTS):\n",
      "   @ MODEL: arywiki_20230101_roberta_mlm_nobots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=10>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 0.0%\n",
      "   @ MODEL: arywiki_20230101_roberta_mlm_nobots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=50>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 0.0%\n",
      "   @ MODEL: arywiki_20230101_roberta_mlm_nobots\n",
      "   @ PARAMS: <vocab_size=52,000, vector_size=514, top_k=100>\n",
      "     * FILE: Masked_Arab_States_Dataset_All.csv\n",
      "       > ACCURACY: 0.62%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlm_utils import *\n",
    "from pathlib import Path\n",
    "import os, logging, warnings\n",
    "from transformers import pipeline\n",
    "from transformers import logging as hflogging\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "\n",
    "disable_progress_bars()\n",
    "hflogging.set_verbosity_error()\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "models = ['SaiedAlshahrani/arywiki_20230101_roberta_mlm_nobots']\n",
    "\n",
    "files = [str(x) for x in Path('.').glob('MASD/Masked_Arab_States_Dataset_All.csv')]\n",
    "\n",
    "print(f\"## EVALUATION OF MOROCCAN ARABIC WIKIPEDIA MASKED LANGUAGE MODEL (WITH NO BOTS):\")\n",
    "\n",
    "top_ks=[10, 50, 100]\n",
    "dataframes = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for model in models:\n",
    "        model_name = str(model.split('/')[-1])\n",
    "        print(f\"   @ MODEL: {model_name}\")\n",
    "        \n",
    "        model_params = f'<vocab_size={format(get_model_config(model)[\"vocab_size\"],\",d\")}, vector_size={get_model_config(model)[\"max_position_embeddings\"]}, top_k={top_k}>'\n",
    "        print(f\"   @ PARAMS: {model_params}\")\n",
    "\n",
    "        for file in files:\n",
    "            dataframe = []\n",
    "            dataframe.append(model_name)\n",
    "            dataframe.append(model_params)\n",
    "\n",
    "            masked_test_set = pd.read_csv(file)\n",
    "            masked_test_set['pred_masked_tokens'] = masked_test_set.apply(lambda col: mask_filler(col['mlm_prompt'], model, top_k), axis=1)\n",
    "            masked_test_set['is_accurate'] = masked_test_set.apply(lambda row: 1 if row.masked_token in row.pred_masked_tokens else 0, axis=1)\n",
    "            accuracy = (masked_test_set['is_accurate'].sum()/masked_test_set.shape[0])*100\n",
    "\n",
    "            print(f\"     * FILE: {file.split('/')[1]}\")\n",
    "            dataframe.append(file)\n",
    "\n",
    "            print(f'       > ACCURACY: {round(accuracy, 2)}%')\n",
    "            dataframe.append(round(accuracy, 2))\n",
    "\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "dataframes = pd.DataFrame(dataframes)\n",
    "dataframes = dataframes.rename(columns={0: 'Model', 1: 'Params',  2: 'File', 3: 'Accuracy'})\n",
    "dataframes.to_csv('arywiki_mlm_results_nobots.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (mldl)",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
